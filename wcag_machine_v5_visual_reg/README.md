# WCAG Machine â€“ Agentic Accessibility Compliance Pipeline

`wcag_machine` implements a modular, agentâ€‘oriented pipeline for discovering Web Content Accessibility Guidelines (WCAG) violations, orchestrating outreach to site owners and deploying a dashboard to visualise compliance.  It combines automated search, scanning, enrichment, drafting and deployment tasks into separate agents that can be composed or replaced independently.  The project draws inspiration from the agentic orchestration pack described in the Kimi collaborationã€450866152419010â€ screenshotã€‘.

## Architecture Overview

The pipeline is broken into **five small agents** that communicate via Redis and can run in parallel.  Each agent exposes a simple commandâ€‘line interface, a health endpoint and a builtâ€‘in test, enabling them to be orchestrated by scripts or other agents without tightly coupling codeã€450866152419010â€ screenshotã€‘.  The highâ€‘level flow looks like this:

1. **Keyword â†’ URLs (AgentÂ 1)** â€“ Given a search keyword, fetch up to 100 organic search result links using the [SerpApi](https://serpapi.com/) and push them into a Redis queue.  Output JSON summarises how many URLs were queued.
2. **Scan 1Â URL (AgentÂ 2)** â€“ Pop a URL from the queue and launch a headless browser with axeâ€‘core to measure WCAG violations.  The full report is stored in Redis and the number of violations is printedã€488929966257325â€ screenshotã€‘.
3. **Mine CEO (AgentÂ 3)** â€“ Derive the CEOâ€™s contact information from the domain (placeholder implementation) and create a HubSpot contact.  Returns the contact ID and emailã€264826787727789â€ screenshotã€‘.
4. **Draft Outreach (AgentÂ 4)** â€“ Generate a personalised eâ€‘mail using the scan report and CEO details.  This module is a stub; integrate your favourite LLM or templating engine via `lib/draft.js`.
5. **Deploy & Health (AgentÂ 5)** â€“ Publish the live dashboard (e.g. to Render or Railway) and return a URLã€271340365084928â€ screenshotã€‘.

These agents are orchestrated by `orchestrate.sh`, which spawns each service concurrently and waits for all to succeed before declaring the pipeline live.  Because each service is â‰¤50Â lines and selfâ€‘testsã€450866152419010â€ screenshotã€‘, you can swap, scale or schedule them independently.  In addition to runtime orchestration, this repository includes a **GitHub Actions workflow** that automates visual regression tests on every push or pull request.  The workflow lives at `.github/workflows/visual-regression.yml` and runs the capture script, checks the results against the golden baseline and uploads artefacts for review.  When a PR is labelled `update-golden` the workflow will automatically update the `test-golden` baseline with the new screenshots.

### Visual Regression Testing

The repository includes a **visual regression workflow** to guard against unintended UI changes.  The `scripts/captureâ€‘all.js` script drives Playwright to take screenshots of key pages (dashboard, review modal, etc.) and writes them into `testâ€‘artefacts`.  The Jestâ€‘style test `test/visualâ€‘capture.test.js` uses `pixelmatch` and `pngjs` to compare live captures against committed baseline images within a 2Â % threshold.  On CI, the `visualâ€‘regression.yml` workflow runs these steps on every push or PR, uploads artefacts for inspection and, when the `updateâ€‘golden` label is applied to a pull request, commits the new screenshots into the `testâ€‘golden` branchã€138200628228501â€ screenshotã€‘.

### Repository Structure

```text
wcag_machine/
â”œâ”€â”€ agentâ€‘keyword.service.js       # AgentÂ 1: fetch SERP results and queue URLs
â”œâ”€â”€ agentâ€‘scan.service.js          # AgentÂ 2: run axeâ€‘core scan on a URL
â”œâ”€â”€ agentâ€‘ceo.service.js           # AgentÂ 3: find CEO contact and create HubSpot contact
â”œâ”€â”€ agentâ€‘draft.service.js         # AgentÂ 4: draft personalised eâ€‘mail (stub)
â”œâ”€â”€ agentâ€‘deploy.service.js        # AgentÂ 5: deploy dashboard (stub)
â”œâ”€â”€ orchestrate.sh                 # Shell orchestrator to run all agents in parallel
â”œâ”€â”€ scripts/captureâ€‘all.js         # Playwright script to capture UI screenshots
â”œâ”€â”€ test/visualâ€‘capture.test.js    # Visual diff test using pixelmatch
â”œâ”€â”€ testâ€‘artefacts/                # Generated screenshots (tracked via .gitkeep)
â”œâ”€â”€ testâ€‘golden/                   # Committed baseline images for comparison
â”œâ”€â”€ lib/                           # Helper modules (redis, serpapi, scan, ceo, draft, hubspot, deploy)
â”œâ”€â”€ .github/workflows/visualâ€‘regression.yml  # CI workflow for visual regression
â””â”€â”€ README.md
```

## Installation

1. **Prerequisites:**
   * [Node.jsÂ â‰¥Â 20](https://nodejs.org/) and `npm`.
   * A [Redis](https://upstash.com/) database; the scripts use [Upstash](https://upstash.com/) via REST.
   * API keys for SerpApi (`SERPAPI_KEY`) and HubSpot (`HUBSPOT_API_KEY`).
   * Optional: `UPSTASH_REDIS_REST_URL` and `UPSTASH_REDIS_REST_TOKEN` for Redis; `DEPLOY_URL` for deployment.

2. **Clone and install dependencies:**

   ```bash
   git clone <yourâ€‘repo>
   cd wcag_machine
   npm install
   ```

3. **Set environment variables:**  Create an `.env` file or export variables in your shell:

   ```bash
   export SERPAPI_KEY=your_serpapi_key
   export UPSTASH_REDIS_REST_URL=your_upstash_url
   export UPSTASH_REDIS_REST_TOKEN=your_upstash_token
   export HUBSPOT_API_KEY=your_hubspot_key
   export DEPLOY_URL=https://yourâ€‘dashboard.onrender.com
   ```

## Usage

### Running Agents Individually

Each agent is a standalone script.  For example, to fetch URLs for the keyword â€œoilâ€:

```bash
node agentâ€‘keyword.service.js oil
# â†’ {"ok":true,"count":100,"urls":[...]}
```

To scan a specific URL:

```bash
node agentâ€‘scan.service.js https://example.com
# â†’ {"ok":true,"url":"https://example.com","violations":12}
```

### Orchestrated Pipeline

Run the entire pipeline with a single command.  The orchestrator takes a keyword, launches all agents concurrently and waits until they complete:

```bash
bash orchestrate.sh oil
```

If every agent exits successfully the script prints:

```
ğŸš€ All agents passed â€“ pipeline live
```

### Visual Regression

To generate fresh screenshots of your UI and compare them with the baseline:

```bash
npm run capture
npm run test:visual
```

If you change the UI intentionally, update the baselines by copying files from `testâ€‘artefacts` into `testâ€‘golden` and committing them.

### Deployment

The deployment agent is a stub that simply echoes `DEPLOY_URL`.  To deploy a real dashboard you can follow the [Zeroâ€‘Budget outreach pipeline guide]ã€628026163413504â€ screenshotã€‘, which shows how to build a minimal Express server and publish it to Render with a single API call.  For containerised projects, see the Dockerfile & Railway deployment guideã€96249953111154â€ screenshotã€‘.

## What's New in v5.0

This release (v5.0) integrates productionâ€‘grade enhancements drawn from the Kimi collaboration.  Highlights:

* **Persistent job queue:** When `USE_BULLMQ=true` the keyword agent enqueues URLs onto a BullMQ queue namespaced by `TENANT_ID` (e.g. `t:default:queue:scan`).  The new `agent-scan-worker.service.js` listens on this queue and processes scans concurrently.  Legacy Redis list mode is still supported.
* **Tenantâ€‘aware namespaces:** All Redis keys and queue names are automatically prefixed with `t:<TENANT_ID>` via `lib/tenant.js` to isolate customer data.  Set `TENANT_ID` in your environment; defaults to `default`.
* **Database persistence (stub):** Scan reports and badges can be persisted to Postgres via `lib/db.js` when `DATABASE_URL` is defined.  If no database is configured the persistence functions noâ€‘op and log a warning.
* **Reâ€‘audit & remediation loop:** The new `agent-replay.service.js` implements a closed loop: reâ€‘scan pages, compare against `VIOLATION_THRESHOLD`, mint a WCAG badge via `lib/badge.js` and enqueue followâ€‘up jobs.  `agent-badge.service.js` consumes badge jobs and logs badge URLs for integration with eâ€‘mail or CRM systems.
* **Multiâ€‘tenant SaaS plumbing:** A minimal SaaS model is includedâ€”tenant IDs are loaded from environment, and queues/keys are namespaced.  The foundation is laid for billing tiers and perâ€‘tenant rate limiting.
* **UI quality checklist:** See the â€œUIâ€¯STILLâ€‘WORKSâ€ checklist near the end of this document for guidance on visual, accessibility and performance regression tests to run in CI.

### New environment variables

| Variable | Description |
| --- | --- |
| `USE_BULLMQ` | Set to `true` to enable BullMQ job queues.  Falls back to simple Redis lists when unset. |
| `TENANT_ID` | Namespace for queues and keys (default `default`). |
| `REDIS_HOST`, `REDIS_PORT`, `REDIS_PASSWORD` | Connection info for BullMQ queues when using a standard Redis server. |
| `DATABASE_URL` | PostgreSQL connection string used by `lib/db.js` to persist scan reports and badges. |
| `VIOLATION_THRESHOLD` | Maximum number of violations allowed when minting a badge during reâ€‘audit (default `5`). |

## Extending to a Full v5.0 App

This repository demonstrates the skeleton of a WCAG compliance machine, but a productionâ€‘grade v5.0 application would need additional features.  Below are key enhancements and how to implement them:

| Area | Missing Feature | How to Add |
| --- | --- | --- |
| **Persistent queue** | **Implemented in v5.0.**  URLs are pushed onto a BullMQ queue (when `USE_BULLMQ=true`) and processed by `agent-scan-worker.service.js`.  Redis list mode remains for quick demos. | N/A â€“ already integrated. |
| **Scan scheduler** | The pipeline scans only a single keyword on demand.  A v5.0 app should schedule recurring scans across multiple keywords and domains. | Add a scheduler (cron job or [node-cron](https://www.npmjs.com/package/node-cron)) that periodically invokes the orchestrator with different keywords.  Persist scan metadata (scan ID, timestamp, keyword) in a database so you can track history. |
| **Storage & analytics** | **Partially implemented.**  v5.0 includes a Postgres persistence stub (`lib/db.js`) which stores scan reports and badges when `DATABASE_URL` is configured.  A full analytics dashboard remains to be built. | Use `lib/db.js` to connect to Postgres or ClickHouse.  Build a frontâ€‘end that queries aggregated metrics (e.g. violations per guideline) and displays trends over time. |
| **Authentication & multiâ€‘user support** | **Multiâ€‘tenant groundwork added.**  Keys and queue names are namespaced by `TENANT_ID` to isolate customers.  Authentication and perâ€‘user dashboards still need to be integrated. | Integrate an auth provider (e.g. Auth0, Clerk) and associate scans/contacts with user accounts in the database. |
| **Real CEO enrichment** | `mineCeo` returns a dummy contact.  Use a real enrichment API. | Integrate [Hunter.io](https://hunter.io/) or [Clearbit](https://clearbit.com/) to fetch company decisionâ€‘makers.  Handle rate limits and fallbacks. |
| **Automated outreach** | The draft agent is a stub.  For highâ€‘quality eâ€‘mails, call a language model (e.g. OpenAIâ€™s GPTâ€‘4) with the scan summary and tone instructions. | Add a `lib/llm.js` module that calls the OpenAI API.  Template the message to highlight specific WCAG violations and recommended fixes. |
| **Live viewer** | The example viewer in the zeroâ€‘budget pipeline uses a simple WebSocket to stream scan progressã€628026163413504â€ screenshotã€‘. | Expand it into a realâ€‘time dashboard: use WebSockets or serverâ€‘sent events to display the queue status, current URL being scanned and violations found so far. |
| **CI/CD pipeline** | Only visual regression tests run in CI.  Add unit tests, linting and deployment automation. | Extend `.github/workflows` with steps for `npm run test` (Jest or Vitest), ESLint, Prettier and automatic deployment via the deploy agent. |
| **Internationalisation & accessibility** | The dashboard itself must meet WCAG 2.2.  | Use semantic HTML, ARIA attributes and the guidance from the W3C evaluation toolsã€138200628228501â€ screenshotã€‘.  Add i18n support via libraries like `reactâ€‘i18next`. |
| **Traumaâ€‘informed design** | If your outreach touches sensitive content, incorporate traumaâ€‘informed principles such as consent, transparency and psychological safety. | Include clear optâ€‘out mechanisms in eâ€‘mails, avoid shaming language and provide educational resources about accessibility. |

## Evaluation Meta Prompts

Use the following 20 prompts to test and interrogate your WCAG Machine.  They exercise the pipelineâ€™s core functions, error handling and extensibility.  You can run them as manual checks or feed them into an agentic testing harness:

1. **Basic SERP ingestion:** â€œFetch the top 10 results for the keyword `wheelchair ramp design` and return their URLs.â€
2. **Scan a simple site:** â€œScan `https://example.com` for WCAG violations and list the first five violations with their impact level.â€
3. **Invalid URL handling:** â€œAttempt to scan `htp://invalid-url` and describe how the agent reports the error.â€
4. **Keyword with no results:** â€œSearch for `asdfqwerzxcv1234` and explain what happens when no SERP results are found.â€
5. **Large result set:** â€œQueue and scan the top 100 results for `accessibility consulting` and summarise the average number of violations.â€
6. **CEO enrichment:** â€œGiven the domain `mozilla.org`, mine the CEOâ€™s contact and show the output JSON.â€
7. **HubSpot integration:** â€œSimulate creating a new contact for `example.org` and verify that the contact ID is returned.â€
8. **Draft generation:** â€œGenerate a personalised outreach eâ€‘mail for `wcagtracker.com` with a friendly yet professional tone.â€
9. **Pipeline orchestration:** â€œRun the full pipeline on the keyword `electric cars` and report when all agents have completed.â€
10. **Deployment check:** â€œDeploy the dashboard and return the URL where the results can be viewed.â€
11. **Visual regression happy path:** â€œCapture the dashboard and compare it against the golden baseline; confirm that the visual drift is under 2Â %.â€
12. **Visual regression failure:** â€œIntentionally change the dashboardâ€™s primary colour and run the visual test; observe how the failure is reported.â€
13. **Scaling test:** â€œLaunch three concurrent scans for the keywords `air quality`, `accessible travel` and `braille literacy`; ensure that the queue and agents handle parallel workloads.â€
14. **Scheduler integration:** â€œSchedule weekly scans for `openai.com` and `github.com` and show where the next run time is stored.â€
15. **Database persistence:** â€œSave the violations for `example.com` into a Postgres database and query the total count of critical issues.â€
16. **API error recovery:** â€œSimulate SerpApi returning a 429 rate limit error and demonstrate how the keyword agent retries or backs off.â€
17. **Security scanning:** â€œEnsure that the scan agent properly isolates untrusted pages and cannot access the host file system.â€
18. **International site:** â€œScan a nonâ€‘English site such as `https://beeline.ru` and verify that the report still identifies WCAG violations.â€
19. **CI integration:** â€œDescribe the CI workflow that runs capture and visual tests on every push and uploads artefacts for review.â€
20. **Traumaâ€‘informed outreach:** â€œRewrite the outreach eâ€‘mail for `mentalhealth.org` with sensitivity to traumaâ€‘informed language and consent.â€

## UIâ€¯â€œSTILLâ€‘WORKSâ€ Checklist

To prevent regressions as the repository scales, adopt the following guardrails in your CI and production monitoring.  Each bullet should have an owner, an automated test and an alert:

1. **Visual regression guardrails**
   * Run `npm run capture` to produce golden PNGs for `/dashboard`, `/report` and `/onboarding`.
   * Configure a GitHub workflow to run `pixelmatch` on every PR and fail if the diff exceeds 0.2â€¯%.
   * Store baselines in the `test-golden` branch and lock PR merges until baselines are updated via an `update-golden` comment.

2. **Accessibility regression guardrails**
   * In the same CI job, run `axe-core/playwright` against the built dashboard (e.g. `localhost:4173`).
   * Allow zero new WCAGÂ 2.2Â AA violations; fail the PR when any are detected.
   * Publish the annotated HTML report as a CI artefact for review.

3. **Cross-browser matrix**
   * Use Playwright to test Chromium, Firefox and WebKit on NodeÂ 20.
   * Add Safariâ€“iOS and Chromeâ€“Android emulators in a GitHub Actions large runner once per day (not per PR to save build minutes).

4. **Responsive snapâ€‘points**
   * Cover breakpoints 320,â€¯768,â€¯1280 andâ€¯1920 pixels in visual tests.
   * Use fullâ€‘page screenshots (`locator.screenshot({ fullPage: true })`) so sticky headers donâ€™t clip.

5. **Performance budget**
   * Enforce a Lighthouse CI gate: LargestÂ ContentfulÂ PaintÂ â‰¤Â 2.5â€¯s and TimeÂ toÂ InteractiveÂ â‰¤Â 3â€¯s.
   * Ensure the dashboard remains fast even with 10â€¯k rows of data.

## GitHub Migration

This repository is prepared within the container under `/home/oai/share/wcag_machine`.  To migrate it to GitHub:

1. **Create a new repository** named `wcag_machine` on your GitHub account.
2. **Initialise git** in the project directory, add all files and commit:

   ```bash
   cd wcag_machine
   git init
   git add .
   git commit -m "Initial commit â€“ WCAG Machine v5.0"
   git remote add origin https://github.com/<yourâ€‘username>/wcag_machine.git
   git push -u origin main
   ```

Due to current connector restrictions the assistant cannot push directly to GitHub on your behalf.  Once the repository is created, you can enable GitHub Actions for continuous testing and deployment.

## License

This project is provided asâ€‘is for educational purposes.  You are responsible for ensuring compliance with the licenses of thirdâ€‘party services used (SerpApi, Upstash, HubSpot, etc.).